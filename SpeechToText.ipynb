{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4xI_Itm5ki8x"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "booR5pLhi_MP"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "123f22540a3d47a7b2bb5499fac3abef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)v2/resolve/main/preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shaur\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.18.0-py3.8.egg\\huggingface_hub\\file_download.py:138: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shaur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f7d66295928476fa9c86b378a48e957",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)ge-v2/resolve/main/tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a54a7f2845a4509b697a153589b28ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)whisper-large-v2/resolve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ca320fd36cd4feda7af0cbbcc021419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)per-large-v2/resolve/main/tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "963aa4f791d544eabd0e285835be9f34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)whisper-large-v2/resolve/main/merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec73d3a63f664442bacc6557344ad5cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)er-large-v2/resolve/main/normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d73c32606d794cfb94b0669c51df8880",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-large-v2/resolve/main/added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f0f7a882d074e4d8ca274e4424eabd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-v2/resolve/main/special_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8211ad63a7ad446690761b9d79132ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)hisper-large-v2/resolve/main/config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b028b55b4e344689038bc863a999e12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize the processor and model\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
        "model.config.forced_decoder_ids = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrvr_uJtjxPH"
      },
      "outputs": [],
      "source": [
        "audio_file_path = \"sample.wav\"\n",
        "audio_input, sampling_rate = sf.read(audio_file_path)\n",
        "audio_input = audio_input.T  # Transpose the audio input if needed\n",
        "sampling_rate = int(sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqiIlDWxky5U"
      },
      "outputs": [],
      "source": [
        "audio_input = torch.tensor(audio_input)\n",
        "\n",
        "# Generate token IDs\n",
        "input_features = processor(audio_input, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
        "predicted_ids = model.generate(input_features)\n",
        "\n",
        "# Decode token IDs to text\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO8_N7kplAtC",
        "outputId": "aac5b0e7-eb2c-4d65-b14d-df1d071e409b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription:\n",
            " I'll tell you basically what this is about is when I was watching Harvey McKay at one of Harv Ecker's things, he said he just finished the Boston Marathon and you know the guy is 76 and I went holy crap, you know, that's amazing. He looked so fit and he's so quick-minded and so on. I thought, all of a sudden it occurred to me, I bet the way you eat, you know, is different. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage. I bet you don't just eat a bunch of garbage.\n"
          ]
        }
      ],
      "source": [
        "# Print the transcription\n",
        "print(\"Transcription:\")\n",
        "print(transcription[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtDJBmzIpTfX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
